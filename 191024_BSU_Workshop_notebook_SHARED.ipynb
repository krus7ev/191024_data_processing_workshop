{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "191024_BSU_Workshop_notebook_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNzPiFltAQDK",
        "colab_type": "text"
      },
      "source": [
        "# `Reading .csv file from web (gitHub) link`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klLfD6_IClMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Read csv file directly from github\n",
        "data = pd.read_csv(\"https://github.com/krus7ev/191024_data_processing_workshop/raw/master/191021_sentiment_test_csv_teXprocessed.csv\")\n",
        "\n",
        "print(\"\\nDATA COLUMNS:\")\n",
        "print(data.columns)\n",
        "\n",
        "print(\"\\nDATA HEADER:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nDATA DESCRIPTION:\")\n",
        "print(data.describe())\n",
        "\n",
        "print(\"\\nDATA SHAPE: \", data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "809k20zQ437S",
        "colab_type": "text"
      },
      "source": [
        "# `Uploading (.xlsx) files`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8--1kFOQ42-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEYeFi8ABIM6",
        "colab_type": "text"
      },
      "source": [
        "# `Reading .xlsx files filtering and exporting back to .csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiIfiKSAeBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import openpyxl as oxl\n",
        "import pandas   as pd\n",
        "import numpy    as np\n",
        "import io\n",
        "\n",
        "wb_data = oxl.load_workbook(io.BytesIO(uploaded['191022_propaganda_task1_train_truncated_11986.xlsx']))\n",
        "print(\"\\n\", wb_data.sheetnames)\n",
        "\n",
        "dataSheet = wb_data['191022_task1_train_truncated_11986']\n",
        "\n",
        "cell_names = []\n",
        "for cell in dataSheet[1] :\n",
        "    cell_names.append(cell.value) \n",
        "print(\"\\n\", cell_names)\n",
        "\n",
        "dataFrame = pd.DataFrame(dataSheet.values)\n",
        "print(dataFrame.head)\n",
        "\n",
        "# Convert to numpy\n",
        "dataNumpy = dataFrame.get_values()\n",
        "\n",
        "# Extract propaganda and non-prpopaganda annotations for separate analysis\n",
        "dataNumpyPropaganda    = dataNumpy[np.where(dataNumpy[:,2] == 'propaganda')]\n",
        "dataNumpyNonPropaganda = dataNumpy[np.where(dataNumpy[:,2] == 'non-propaganda')]\n",
        "\n",
        "print(\"\\nFirst propaganda sample     :\\n\", dataNumpyPropaganda[0])\n",
        "print(\"\\nFirst non-propaganda sample :\\n\", dataNumpyNonPropaganda[0])\n",
        "\n",
        "print(\"\\n\"+\"dataNumpyPropaganda    SHAPE : \", dataNumpyPropaganda.shape)\n",
        "print(\"\\n\"+\"dataNumpyNonPropaganda SHAPE : \", dataNumpyNonPropaganda.shape)\n",
        "\n",
        "# Translate pandas data frame\n",
        "dataFramePropaganda    = pd.DataFrame(dataNumpyPropaganda,    columns = cell_names)\n",
        "dataFrameNonPropaganda = pd.DataFrame(dataNumpyNonPropaganda, columns = cell_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlghhMDQBuTp",
        "colab_type": "text"
      },
      "source": [
        "# `Text data cleaning and pre-processing`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA5mjaMcISBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    \n",
        "    # Remove special chars and punctuation\n",
        "    text = \" \".join(tokenizer(text))\n",
        "    \n",
        "    # lowcase\n",
        "    text = text.lower()\n",
        "    text = text.split(\" \")\n",
        "    \n",
        "    # Remove stopwords\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    \n",
        "    # Lematize\n",
        "    #text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    \n",
        "    # Lematize - \"v\"? - verb forms...?\n",
        "    #text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwhj0Xg1bf68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "\n",
        "def cleanTexts(texts_list, filters='!\"#$%&()*+,-./:;0123456789<=>?@[\\\\]^_`{|}~\\t\\n', split=\" \") :\n",
        "    texts_clear = []\n",
        "\n",
        "    for text in texts_list :\n",
        "        translate_dict = dict((c, split) for c in filters)\n",
        "        translate_map  = str.maketrans(translate_dict)\n",
        "        text_clear     = text.translate(translate_map)\n",
        "\n",
        "        text_clear = re.sub(r'[\\s]+', ' ', text_clear)\n",
        "\n",
        "        texts_clear   += [text_clear]\n",
        "\n",
        "    return texts_clear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib_QUexeBtXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem                       import WordNetLemmatizer\n",
        "from nltk.corpus                     import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "print(\"\\n\"+\"dataFramePropaganda    SHAPE : \", dataFramePropaganda.shape)\n",
        "print(\"\\n\"+\"dataFrameNonPropaganda SHAPE : \", dataFrameNonPropaganda.shape)\n",
        "\n",
        "\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "cvec = CountVectorizer()\n",
        "tokenizer = cvec.build_tokenizer()\n",
        "\n",
        "dataFramePropaganda['article_prep']    = dataFramePropaganda['article'].apply(clean_text)\n",
        "dataFrameNonPropaganda['article_prep'] = dataFrameNonPropaganda['article'].apply(clean_text)\n",
        "\n",
        "print(dataFrameNonPropaganda['article_prep'].loc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV8SgswVM-62",
        "colab_type": "text"
      },
      "source": [
        "# `Data exploration`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pg1WbGvM1Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud   import WordCloud\n",
        "\n",
        "propaganda_texts     = dataFramePropaganda['article_prep'].get_values()\n",
        "non_propaganda_texts = dataFrameNonPropaganda['article_prep'].get_values()\n",
        "\n",
        "print(propaganda_texts.shape)\n",
        "print(non_propaganda_texts.shape)\n",
        "\n",
        "propaganda_texts     = cleanTexts(propaganda_texts.tolist())\n",
        "non_propaganda_texts = cleanTexts(non_propaganda_texts.tolist())\n",
        "\n",
        "\n",
        "# Join the different processed titles together.\n",
        "flat_propaganda_texts     = ' '.join(propaganda_texts)\n",
        "flat_non_propaganda_texts = ' '.join(non_propaganda_texts)\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=500, contour_width=10, contour_color='steelblue', width=800, height=400)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrnI3v8TSm9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a word cloud\n",
        "wordcloud.generate(flat_propaganda_texts)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMOa2TCmSqmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a word cloud\n",
        "wordcloud.generate(flat_non_propaganda_texts)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq5QiYA5_hXE",
        "colab_type": "text"
      },
      "source": [
        "# `Applying unsupervised analysis`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkz4FGPzULJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_10_most_common_words(count_data, count_vectorizer):\n",
        "    import matplotlib.pyplot as plt\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    total_counts = np.zeros(len(words))\n",
        "    for t in count_data:\n",
        "        total_counts+=t.toarray()[0]\n",
        "    \n",
        "    count_dict = (zip(words, total_counts))\n",
        "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
        "    words = [w[0] for w in count_dict]\n",
        "    counts = [w[1] for w in count_dict]\n",
        "    x_pos = np.arange(len(words)) \n",
        "    \n",
        "    plt.figure(2, figsize=(15, 15/1.6180))\n",
        "    plt.subplot(title='10 most common words')\n",
        "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
        "    sns.barplot(x_pos, counts, palette='husl')\n",
        "    plt.xticks(x_pos, words, rotation=90) \n",
        "    plt.xlabel('words')\n",
        "    plt.ylabel('counts')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS-ciB-B92c_",
        "colab_type": "text"
      },
      "source": [
        "`Top 10 Propaganda terms`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwlYddZcAG15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "import seaborn as sns\n",
        "#%matplotlib inline\n",
        "\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialise a count vectorizer with the English stop words\n",
        "count_vectorizer_PTs = CountVectorizer(max_features  = 23000,\n",
        "                                       stop_words    ='english',\n",
        "                                       token_pattern = r\"(?u)\\b\\w+\\b\",\n",
        "                                       ngram_range   = (1,3),\n",
        "                                       min_df        = 0.005,\n",
        "                                       binary        = False)\n",
        "\n",
        "# Fit and transform the processed titles\n",
        "cv_propaganda_texts = count_vectorizer_PTs.fit_transform(propaganda_texts)\n",
        "\n",
        "\n",
        "# Initialise a term-frequency/inverse-doc-freq vectorizer with the English stop words\n",
        "tfidf_vectorizer_PTs = TfidfVectorizer(max_features  = 23000,\n",
        "                                       stop_words    ='english',\n",
        "                                       token_pattern = r\"(?u)\\b\\w+\\b\",\n",
        "                                       ngram_range   = (1,3),\n",
        "                                       min_df        = 0.005,\n",
        "                                       use_idf       = False)\n",
        "\n",
        "tfidf_vectorizer_PTs.fit(propaganda_texts)\n",
        "cv_tfidf_propaganda_texts = tfidf_vectorizer_PTs.transform(propaganda_texts)\n",
        "\n",
        "\n",
        "# Visualise the 10 most common words\n",
        "plot_10_most_common_words(cv_tfidf_propaganda_texts, tfidf_vectorizer_PTs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQhq4woh9858",
        "colab_type": "text"
      },
      "source": [
        "`Top 10 Non-propaganda terms`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3CFowPWkBFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise a count vectorizer with the English stop-words\n",
        "count_vectorizer_NPTs = CountVectorizer(max_features  = 23000,\n",
        "                                        stop_words    ='english',\n",
        "                                        token_pattern = r\"(?u)\\b\\w+\\b\",\n",
        "                                        ngram_range   = (1,3),\n",
        "                                        min_df        = 0.005,\n",
        "                                        binary        = False)\n",
        "\n",
        "# Fit-transform the processed texts\n",
        "cv_non_propaganda_texts = count_vectorizer_NPTs.fit_transform(non_propaganda_texts)\n",
        "\n",
        "\n",
        "# Initialise a tfidf vectorizer as well\n",
        "tfidf_vectorizer_NPTs = TfidfVectorizer(max_features  = 23000,\n",
        "                                        stop_words    ='english',\n",
        "                                        token_pattern = r\"(?u)\\b\\w+\\b\",\n",
        "                                        ngram_range   = (1,3),\n",
        "                                        min_df        = 0.005,\n",
        "                                        use_idf       = True)\n",
        "\n",
        "# Fit and transform the processed texts\n",
        "tfidf_vectorizer_NPTs.fit(non_propaganda_texts)\n",
        "cv_tfidf_non_propaganda_texts = tfidf_vectorizer_NPTs.transform(non_propaganda_texts)\n",
        "\n",
        "\n",
        "# Visualise the 10 most common words\n",
        "plot_10_most_common_words(cv_tfidf_non_propaganda_texts, tfidf_vectorizer_NPTs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCrwGsyeimnl",
        "colab_type": "text"
      },
      "source": [
        "# `Basic count-based clustering: K-means`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPxQYW5eiuwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "true_k = 5\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(cv_tfidf_propaganda_texts)\n",
        "\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = tfidf_vectorizer_PTs.get_feature_names()\n",
        "for i in range(true_k):\n",
        "    print(\"\\nCluster %d:\" % (i+1)),\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms[ind]),\n",
        "    print\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Prediction\")\n",
        "Y = tfidf_vectorizer_PTs.transform([\"High court judge ruled out Kavanaugh pleading guilty\"])\n",
        "prediction = model.predict(Y)\n",
        "print(prediction)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAVAw0FEgb2d",
        "colab_type": "text"
      },
      "source": [
        "# `Latent Dirichlet Аllocation for Topic modelling`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AesSFYx_g6fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_topics(model, count_vectorizer, n_top_words):\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"\\nTopic #%d:\" % topic_idx)\n",
        "        print(\" \".join([words[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFYc6QDYga9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n",
        "# Load the LDA model from sk-learn\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        " \n",
        "        \n",
        "# Tweak the two parameters below\n",
        "number_topics = 5\n",
        "number_words = 10\n",
        "\n",
        "# Create and fit the LDA model\n",
        "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
        "\n",
        "lda.fit(cv_propaganda_texts)\n",
        "\n",
        "# Print the topics found by the LDA model\n",
        "print(\"Topics found via LDA:\")\n",
        "print_topics(lda, count_vectorizer_PTs, number_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qLgzegSoEmc",
        "colab_type": "text"
      },
      "source": [
        "# `Visualise LDA`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygga3FXYoE02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis\n",
        "\n",
        "from   pyLDAvis import sklearn as sklearn_lda\n",
        "import pyLDAvis\n",
        "import pickle \n",
        "from   datetime import datetime\n",
        "\n",
        "timestamp_lda = datetime.now().strftime(\"%y%m%d_%H%Mh\")\n",
        "LDAvis_data_filepath = (timestamp_lda +'_LDA_vis_prep_'+ str(number_topics) + '_topics.pkl' )\n",
        "LDAvis_prep_filepath = '191023_1150h_LDA_vis_prep_5_topics.pkl'\n",
        "PREP_VIS = False\n",
        "\n",
        "\n",
        "# This is a bit time consuming \n",
        "# - make the if statement True if you want to execute visualization prep yourself\n",
        "if PREP_VIS is True :\n",
        "    LDAvis_prepared = sklearn_lda.prepare(lda, cv_propaganda_texts, count_vectorizer_PTs)\n",
        "    \n",
        "    with open(LDAvis_data_filepath, 'wb') as outf:\n",
        "        pickle.dump(LDAvis_prepared, outf)\n",
        "        outf.close()\n",
        "    \n",
        "# We can use the pre-prepared pyLDAvis file here to load the data from disk\n",
        "else :\n",
        "    with open(LDAvis_prep_filepath, 'rb') as f :\n",
        "        LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, timestamp_lda +'_LDA_vis_'+ str(number_topics) + '_topics.html')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pneqx_Z5TvTZ",
        "colab_type": "text"
      },
      "source": [
        "# `Vectorize full dataset in compact scipy representation`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lSXA1QYTvnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse            import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use same dataframe-numpy-list pattern again for all texts and labels (repeat same steps with some new)\n",
        "dataFrameAllTexts = pd.DataFrame(dataNumpy[1:], columns = cell_names)\n",
        "dataFrameAllTexts['article_prep'] = dataFrameAllTexts['article'].apply(clean_text)\n",
        "dataFrameAllTexts['target']       = dataFrameAllTexts['label'].map({'propaganda': 1, 'non-propaganda': 0})\n",
        "\n",
        "print(dataNumpy[0])\n",
        "print(dataFrameAllTexts['label'].loc[23])\n",
        "print(dataFrameAllTexts['target'].loc[23])\n",
        "\n",
        "all_texts = dataFrameAllTexts['article_prep'].get_values()\n",
        "all_texts = cleanTexts(all_texts.tolist())\n",
        "\n",
        "# Shuffle and split dataset into train and test sets, stratifyig according to its class distribution\n",
        "df_train_data, df_test_data = train_test_split(dataFrameAllTexts, test_size = 0.25, random_state = 42, stratify=dataFrameAllTexts['target'])\n",
        "\n",
        "# Extract train texts and labels\n",
        "train_texts  = df_train_data['article_prep'].get_values()\n",
        "train_texts  = cleanTexts(train_texts.tolist())\n",
        "train_labels = df_train_data['target'].get_values()\n",
        "\n",
        "\n",
        "# Initialise a count vectorizer with the English stop-words\n",
        "count_vectorizer_all = CountVectorizer(max_features  = 23000,\n",
        "                                       stop_words    ='english',\n",
        "                                       token_pattern = r\"(?u)\\b\\w+\\b\",\n",
        "                                       ngram_range   = (1,3),\n",
        "                                       min_df        = 0.005,\n",
        "                                       binary        = False)\n",
        "\n",
        "# Fit vectorizer on all of the processed texts\n",
        "count_vectorizer_all.fit(all_texts)\n",
        "\n",
        "\n",
        "# Vectorize train set\n",
        "cv_train_texts = count_vectorizer_all.transform(train_texts)\n",
        "# Convert to dense representation for csr matrix encoding\n",
        "cv_train_texts_dense = cv_train_texts.todense()\n",
        "\n",
        "\n",
        "# Print out train dataset info\n",
        "all_feature_names = count_vectorizer_all.get_feature_names()\n",
        "print('\\nTOTAL unique ngram-tokens found in texts: %s.\\n' % len(all_feature_names))\n",
        "print('Shape of BOW data mat:', cv_train_texts_dense.shape)\n",
        "\n",
        "\n",
        "# Conert to CSR standard\n",
        "train_texts_csr = csr_matrix(cv_train_texts_dense)\n",
        "train_labels_csr   = [str(i) for i in train_labels]\n",
        "\n",
        "\n",
        "# Visualise the 10 most common words\n",
        "plot_10_most_common_words(cv_train_texts, count_vectorizer_all)\n",
        "\n",
        "\n",
        "##########################################################\n",
        "test_texts  = df_test_data['article_prep'].get_values()\n",
        "test_texts  = cleanTexts(test_texts.tolist())\n",
        "test_labels = df_test_data['target'].get_values()\n",
        "\n",
        "# Vectorize test set too\n",
        "cv_test_texts = count_vectorizer_all.transform(test_texts)\n",
        "# Convert to dense representation for csr matrix encoding\n",
        "cv_test_texts_dense = cv_test_texts.todense()\n",
        "\n",
        "test_texts_csr = csr_matrix(cv_test_texts_dense)\n",
        "test_labels_csr   = [str(i) for i in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzMoKvQDP3qx",
        "colab_type": "text"
      },
      "source": [
        "# `Train supervised text classifiers with sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8KMjXyG6XfP",
        "colab_type": "text"
      },
      "source": [
        "`Decision Tree`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlIt15etP3_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from   sklearn.tree                 import DecisionTreeClassifier\n",
        "from   sklearn.model_selection      import GridSearchCV\n",
        "from   sklearn.metrics              import confusion_matrix\n",
        "from   datetime                     import datetime\n",
        "\n",
        "print(\"\\n===============================================\")\n",
        "print(\"  Training classic best-split Decision Tree\\n\")\n",
        "\n",
        "DecTree = DecisionTreeClassifier()\n",
        "\n",
        "DecTree_params = {'criterion'         : ('gini', 'entropy'),\n",
        "                  'min_samples_split' : (2, 4, 8, 16, 32)}\n",
        "DecTree_search = GridSearchCV(DecTree, DecTree_params, n_jobs=1)\n",
        "\n",
        "DecTree_search.fit(train_texts_csr, train_labels_csr)\n",
        "print(\"DecTree search best score  : \", DecTree_search.best_score_)\n",
        "print(\"DecTree search best params : \", DecTree_search.best_params_)\n",
        "\n",
        "# Store model for future prediction\n",
        "DecTree_best = DecTree_search.best_estimator_\n",
        "\n",
        "outf = open(datetime.now().strftime(\"%y%m%d_%H%Mh\") + \"_DecTree.pkl\", \"wb\")\n",
        "pickle.dump(DecTree_best, outf)\n",
        "outf.close()\n",
        "\n",
        "DecTree_predictions = DecTree_best.predict(test_texts_csr)\n",
        "print(\"First prediction output    : \", DecTree_predictions[0])\n",
        "\n",
        "DecTree_precision = np.mean(DecTree_predictions == test_labels_csr)\n",
        "print(\"Test accuracy score        : \", DecTree_precision)\n",
        "\n",
        "cm_raw = confusion_matrix(test_labels_csr, DecTree_predictions)\n",
        "print('\\nConfusion matrix, without normalization...')\n",
        "print(cm_raw)\n",
        "\n",
        "#Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
        "print('\\nNormalized confusion matrix....')\n",
        "cm_norm = cm_raw.astype('float') / cm_raw.sum(axis=1)[:, np.newaxis]\n",
        "print(cm_norm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLCRbUi6d5D",
        "colab_type": "text"
      },
      "source": [
        "`Naive Bayes Classifier`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d62_Q5Ct6eN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from   sklearn.naive_bayes          import MultinomialNB\n",
        "from   sklearn.model_selection      import GridSearchCV\n",
        "from   sklearn.metrics              import confusion_matrix\n",
        "from   datetime                     import datetime\n",
        "\n",
        "print(\"\\n===============================================\")\n",
        "print(\"  Training Multinomial Naive Bayes Classifier\\n\")\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "\n",
        "MNB_params = {'alpha' : (1.0, 1e-1, 1e-2, 1e-3)}\n",
        "MNB_search = GridSearchCV(MNB, MNB_params, n_jobs=1) #, cv=5) #for k-folds\n",
        "\n",
        "MNB_search.fit(train_texts_csr, train_labels_csr)\n",
        "print(\"DecTree search best score  : \", MNB_search.best_score_)\n",
        "print(\"DecTree search best params : \", MNB_search.best_params_)\n",
        "\n",
        "# Store model for future prediction\n",
        "MNB_best = MNB_search.best_estimator_\n",
        "\n",
        "outf = open(datetime.now().strftime(\"%y%m%d_%H%Mh\") + \"_MNB.pkl\", \"wb\")\n",
        "pickle.dump(MNB_best, outf)\n",
        "outf.close()\n",
        "\n",
        "MNB_predictions = MNB_best.predict(test_texts_csr)\n",
        "print(\"First prediction output    : \", MNB_predictions[0])\n",
        "\n",
        "DecTree_precision = np.mean(MNB_predictions == test_labels_csr)\n",
        "print(\"Test accuracy score        : \", DecTree_precision)\n",
        "\n",
        "cm_raw = confusion_matrix(test_labels_csr, MNB_predictions)\n",
        "print('\\nConfusion matrix, without normalization...')\n",
        "print(cm_raw)\n",
        "\n",
        "#Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
        "print('\\nNormalized confusion matrix....')\n",
        "cm_norm = cm_raw.astype('float') / cm_raw.sum(axis=1)[:, np.newaxis]\n",
        "print(cm_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9iKY9dV6ect",
        "colab_type": "text"
      },
      "source": [
        "`Support Vector Machine`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iOWJLgp6eqa",
        "colab_type": "code",
        "outputId": "585db421-0ece-4dd2-9b2d-869504b37747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from   sklearn.svm                  import SVC\n",
        "from   sklearn.model_selection      import GridSearchCV\n",
        "from   sklearn.metrics              import confusion_matrix\n",
        "from   datetime                     import datetime\n",
        "\n",
        "print(\"\\n===============================================\")\n",
        "print(\"  Training Support Vector Machine (SVC)\\n\")\n",
        "\n",
        "SVC_SVM = SVC()\n",
        "\n",
        "SVC_params = {'C'           : (1.0, 0.1, 0.01, 0.001), #fix with range\n",
        "              'kernel'      : ('rbf', 'linear', 'sigmoid'),\n",
        "              'degree'      : (2, 3, 4)}\n",
        "\n",
        "SVC_search = GridSearchCV(SVC_SVM, SVC_params, n_jobs=1) #, cv=Settings.kFolds)\n",
        "\n",
        "SVC_search.fit(train_texts_csr, train_labels_csr)\n",
        "print(\"SVC search best score: \",  SVC_search.best_score_)\n",
        "print(\"SVC search best params: \", SVC_search.best_params_)\n",
        "\n",
        "# Store model for future prediction\n",
        "SVC_best = SVC_search.best_estimator_\n",
        "\n",
        "outf = open(datetime.now().strftime(\"%y%m%d_%H%Mh\") + \"_MNB.pkl\", \"wb\")\n",
        "pickle.dump(SVC_best, outf)\n",
        "outf.close()\n",
        "\n",
        "SVC_predictions = SVC_best.predict(test_texts_csr)\n",
        "print(\"First prediction output    : \", SVC_predictions[0])\n",
        "\n",
        "SVC_precision = np.mean(SVC_predictions == test_labels_csr)\n",
        "print(\"Test accuracy score        : \", SVC_precision)\n",
        "\n",
        "cm_raw = confusion_matrix(test_labels_csr, SVC_predictions)\n",
        "print('\\nConfusion matrix, without normalization...')\n",
        "print(cm_raw)\n",
        "\n",
        "#Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
        "print('\\nNormalized confusion matrix....')\n",
        "cm_norm = cm_raw.astype('float') / cm_raw.sum(axis=1)[:, np.newaxis]\n",
        "print(cm_norm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===============================================\n",
            "  Training Multinomial Naive Bayes Classifier\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVC search best score:  0.913116030704194\n",
            "SVC search best params:  {'C': 0.01, 'degree': 2, 'kernel': 'linear'}\n",
            "First prediction output    :  0\n",
            "Test accuracy score        :  0.9182515849182515\n",
            "\n",
            "Confusion matrix, without normalization...\n",
            "[[1898   94]\n",
            " [ 151  854]]\n",
            "\n",
            "Normalized confusion matrix....\n",
            "[[0.95281124 0.04718876]\n",
            " [0.15024876 0.84975124]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdugpE_N2JSX",
        "colab_type": "text"
      },
      "source": [
        "# `Extracting web content with Scrapy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czkrJE7c2Y4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scrapy\n",
        "!pip install langdetect\n",
        "!mkdir crawler_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbbjPpN1w-BW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import scrapy\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from langdetect     import detect \n",
        "from google.colab   import drive\n",
        "\n",
        "html_tags = re.compile('<.*?>')\n",
        "\n",
        "\n",
        "class AlbertEinsteinQuotes(scrapy.Spider):\n",
        "    name = \"AlbertEinsteinQuotes\"\n",
        "    start_urls = [\n",
        "        'https://en.wikiquote.org/wiki/Albert_Einstein',\n",
        "    ]\n",
        "    \n",
        "    def stripHTML(self, row):\n",
        "        return re.sub(html_tags, '', row)\n",
        "    \n",
        "    def downloadDF(self, df, dfName):\n",
        "        df.to_csv(\"crawled_data/\" + dfName + '.csv')\n",
        "\n",
        "    \n",
        "    def parse(self, response):\n",
        "        columns = [\"text\"]\n",
        "        values_en, values_de, values_fr = ([] for i in range(3))\n",
        "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
        "            row = self.stripHTML(quote.extract())\n",
        "            text = row.split(\"\\n\")[0]\n",
        "            if detect(text) == 'en':\n",
        "                values_en.append(text)\n",
        "            elif detect(text) == 'de':\n",
        "                values_de.append(text)\n",
        "            elif detect(text) == 'fr':\n",
        "                values_fr.append(text)\n",
        "                \n",
        "        enQuotesDF = pd.DataFrame(values_en, columns=columns)\n",
        "        print(\"English Dataframe Contens \", enQuotesDF[:10], sep='\\n')    \n",
        "        self.downloadDF(enQuotesDF, \"Albert_Einstein_English_Quotes\")\n",
        "        \n",
        "        deQuotesDF = pd.DataFrame(values_de, columns=columns)\n",
        "        print(\"German Dataframe Contens \", deQuotesDF[:10], sep='\\n')    \n",
        "        self.downloadDF(enQuotesDF, \"Albert_Einstein_German_Quotes\")\n",
        "\n",
        "        \n",
        "        frQuotesDF = pd.DataFrame(values_fr, columns=columns)\n",
        "        print(\"French Dataframe Contens \", frQuotesDF[:10], sep='\\n')    \n",
        "        self.downloadDF(enQuotesDF, \"Albert_Einstein_French_Quotes\")\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(AlbertEinsteinQuotes)\n",
        "process.start()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnfNGcU6Acaw",
        "colab_type": "text"
      },
      "source": [
        "# `Download files to local system`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWtT8V90AamR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open('example.txt', 'w') as f:\n",
        "  \n",
        "  f.write('some content')\n",
        "\n",
        "files.download('example.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cs_4n5TU7cC3"
      },
      "source": [
        "# `Mount Google Drive`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGGM_eubZEVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}